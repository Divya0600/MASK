{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj0qcWLk9xBA",
        "outputId": "2bffed15-fea3-4a50-f31f-2fdc3d68a6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting presidio-analyzer\n",
            "  Downloading presidio_analyzer-2.2.357-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting presidio-anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.357-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio-analyzer)\n",
            "  Downloading phonenumbers-8.13.53-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio-analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio-analyzer) (2024.11.6)\n",
            "Collecting tldextract (from presidio-analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core (from presidio-anonymizer)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting pycryptodome>=3.10.1 (from presidio-anonymizer)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core->presidio-anonymizer) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio-analyzer) (3.16.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.357-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading presidio_anonymizer-2.2.357-py3-none-any.whl (31 kB)\n",
            "Downloading phonenumbers-8.13.53-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, pycryptodome, requests-file, azure-core, tldextract, presidio-anonymizer, presidio-analyzer\n",
            "Successfully installed azure-core-1.32.0 phonenumbers-8.13.53 presidio-analyzer-2.2.357 presidio-anonymizer-2.2.357 pycryptodome-3.21.0 requests-file-2.1.0 tldextract-5.1.3\n"
          ]
        }
      ],
      "source": [
        "%pip install presidio-analyzer presidio-anonymizer spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "import os\n",
        "import requests\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "# Disable SSL verification (for your use case)\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "# Load the local SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create Presidio Analyzer and Anonymizer Engines\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "\n",
        "# Pass the model name, not the actual model object\n",
        "nlp_engine = SpacyNlpEngine(models=[{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}])  # Use the model name here\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Function to mask PII entities using Presidio\n",
        "def mask_pii_with_presidio(text):\n",
        "    # Analyze the text to find PII\n",
        "    results = analyzer.analyze(text=text, language=\"en\")\n",
        "\n",
        "    # Print detected entities and their labels\n",
        "    for result in results:\n",
        "        print(f\"Entity: {result.entity_type}, Text: {text[result.start:result.end]}\")\n",
        "\n",
        "    # Anonymize (mask) the detected PII entities\n",
        "    anonymized_text = anonymizer.anonymize(text, results)\n",
        "\n",
        "    return anonymized_text\n",
        "\n",
        "# Sample input text with PII\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Mask the PII from the input text\n",
        "masked_output = mask_pii_with_presidio(input_text)\n",
        "\n",
        "print(\"\\nMasked Output:\\n\", masked_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSELP-qz96uN",
        "outputId": "d3ef6e26-d077-4d36-e544-c43f63944f8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Entity FAC is not mapped to a Presidio entity, but keeping anyway. Add to `NerModelConfiguration.labels_to_ignore` to remove.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: PERSON, Text: John Doe\n",
            "Entity: LOCATION, Text: Springfield\n",
            "Entity: DATE_TIME, Text: 12th July 1990\n",
            "Entity: DATE_TIME, Text: 9 AM to 5 PM\n",
            "\n",
            "Masked Output:\n",
            " text: \n",
            "<PERSON> lives at 123 Elm Street, <LOCATION>. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on <DATE_TIME> and works from <DATE_TIME>.\n",
            "\n",
            "items:\n",
            "[\n",
            "    {'start': 142, 'end': 153, 'entity_type': 'DATE_TIME', 'text': '<DATE_TIME>', 'operator': 'replace'},\n",
            "    {'start': 115, 'end': 126, 'entity_type': 'DATE_TIME', 'text': '<DATE_TIME>', 'operator': 'replace'},\n",
            "    {'start': 35, 'end': 45, 'entity_type': 'LOCATION', 'text': '<LOCATION>', 'operator': 'replace'},\n",
            "    {'start': 1, 'end': 9, 'entity_type': 'PERSON', 'text': '<PERSON>', 'operator': 'replace'}\n",
            "]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from presidio_analyzer import AnalyzerEngine, RecognizerResult, EntityRecognizer\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
        "\n",
        "# Load SpaCy model\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "try:\n",
        "    nlp = spacy.load(spacy_model_path)\n",
        "    print(\"SpaCy model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading SpaCy model: {e}\")\n",
        "\n",
        "# Initialize SpaCy NLP Engine\n",
        "nlp_engine = SpacyNlpEngine(models=[{\"lang_code\": \"en\", \"model_name\": spacy_model_path}])\n",
        "\n",
        "# Custom recognizer for allowlist\n",
        "class AllowlistRecognizer(EntityRecognizer):\n",
        "    def __init__(self, allowlist):\n",
        "        super().__init__()\n",
        "        self.allowlist = allowlist\n",
        "\n",
        "    def load(self):\n",
        "        pass  # No external models to load\n",
        "\n",
        "    def analyze(self, text, entities, nlp_artifacts=None):\n",
        "        results = []\n",
        "        for word in self.allowlist:\n",
        "            start = text.find(word)\n",
        "            if start != -1:\n",
        "                end = start + len(word)\n",
        "                results.append(\n",
        "                    RecognizerResult(entity_type=\"ALLOWLIST\", start=start, end=end, score=1.0)\n",
        "                )\n",
        "        return results\n",
        "\n",
        "# Custom recognizer for denylist\n",
        "class DenylistRecognizer(EntityRecognizer):\n",
        "    def __init__(self, denylist):\n",
        "        super().__init__()\n",
        "        self.denylist = denylist\n",
        "\n",
        "    def load(self):\n",
        "        pass  # No external models to load\n",
        "\n",
        "    def analyze(self, text, entities, nlp_artifacts=None):\n",
        "        results = []\n",
        "        for word in self.denylist:\n",
        "            start = text.find(word)\n",
        "            if start != -1:\n",
        "                end = start + len(word)\n",
        "                results.append(\n",
        "                    RecognizerResult(entity_type=\"DENYLIST\", start=start, end=end, score=1.0)\n",
        "                )\n",
        "        return results\n",
        "\n",
        "# Create allowlist and denylist\n",
        "allowlist = {\"TechCorp\", \"Springfield\"}  # Words to ignore\n",
        "denylist = {\"John\", \"Doe\", \"Elm Street\"}  # Words to always mask\n",
        "\n",
        "# Register custom recognizers\n",
        "registry = RecognizerRegistry()\n",
        "registry.add_recognizer(AllowlistRecognizer(allowlist))\n",
        "registry.add_recognizer(DenylistRecognizer(denylist))\n",
        "\n",
        "# Initialize AnalyzerEngine and AnonymizerEngine\n",
        "analyzer = AnalyzerEngine(registry=registry, nlp_engine=nlp_engine)\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Function to mask PII with allowlist and denylist handling\n",
        "def mask_pii_with_lists(text):\n",
        "    # Analyze the text\n",
        "    results = analyzer.analyze(\n",
        "        text=text,\n",
        "        language=\"en\",\n",
        "        score_threshold=0.5,  # Threshold for PII detection\n",
        "    )\n",
        "\n",
        "    # Filter results based on allowlist/denylist\n",
        "    filtered_results = []\n",
        "    for result in results:\n",
        "        if result.entity_type == \"ALLOWLIST\":\n",
        "            print(f\"Skipping allowlist word: {text[result.start:result.end]}\")\n",
        "            continue\n",
        "        if result.entity_type == \"DENYLIST\":\n",
        "            print(f\"Forcing denylist word: {text[result.start:result.end]}\")\n",
        "        filtered_results.append(result)\n",
        "\n",
        "    # Anonymize (mask) the filtered entities\n",
        "    anonymized_text = anonymizer.anonymize(\n",
        "        text=text,\n",
        "        analyzer_results=filtered_results,\n",
        "        operators={\"DEFAULT\": {\"type\": \"replace\", \"new_value\": \"<MASK>\"}}\n",
        "    )\n",
        "\n",
        "    return anonymized_text.text\n",
        "\n",
        "# Sample text\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Run the masking function\n",
        "masked_output = mask_pii_with_lists(input_text)\n",
        "print(\"\\nMasked Output:\\n\", masked_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "dGGl9ZhS-GRI",
        "outputId": "e08a4595-2126-42e8-886d-2ea87202a6fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy model loaded successfully!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EntityRecognizer.__init__() missing 1 required positional argument: 'supported_entities'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-937013cc22f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Register custom recognizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mregistry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecognizerRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_recognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllowlistRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallowlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_recognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDenylistRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-937013cc22f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, allowlist)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAllowlistRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntityRecognizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallowlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EntityRecognizer.__init__() missing 1 required positional argument: 'supported_entities'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "# Load SpaCy model for NLP\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "nlp = spacy.load(spacy_model_path)\n",
        "\n",
        "# Initialize SpaCy NLP Engine\n",
        "nlp_engine = SpacyNlpEngine(models=[{\"lang_code\": \"en\", \"model_name\": spacy_model_path}])\n",
        "\n",
        "# Initialize Analyzer Engine\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n",
        "\n",
        "# Initialize Anonymizer Engine\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Define Allowlist and Denylist\n",
        "allowlist = [\"TechCorp\", \"Springfield\"]\n",
        "denylist = [\"John\", \"Doe\"]\n",
        "\n",
        "# Mask PII Function\n",
        "def mask_pii(text):\n",
        "    # Analyze the text for PII entities\n",
        "    results = analyzer.analyze(text=text, language=\"en\", score_threshold=0.5)\n",
        "\n",
        "    # Filter out allowlisted entities\n",
        "    filtered_results = [\n",
        "        result for result in results if result.entity_type not in [\"ALLOWLIST\"] and result.entity_type != \"DENYLIST\"\n",
        "    ]\n",
        "\n",
        "    # Anonymize (mask) the text based on filtered results\n",
        "    anonymized_text = anonymizer.anonymize(text=text, analyzer_results=filtered_results)\n",
        "\n",
        "    return anonymized_text.text\n",
        "\n",
        "# Test Input\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Run the function\n",
        "masked_text = mask_pii(input_text)\n",
        "print(\"\\nMasked Output:\\n\", masked_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnj3TkVY_J2U",
        "outputId": "b74c210b-c853-46ae-e420-b79eeb2b8990"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Entity FAC is not mapped to a Presidio entity, but keeping anyway. Add to `NerModelConfiguration.labels_to_ignore` to remove.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Output:\n",
            " \n",
            "<PERSON> lives at 123 Elm Street, <LOCATION>. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on <DATE_TIME> and works from <DATE_TIME>.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "import spacy\n",
        "from presidio_analyzer import RecognizerRegistry\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger(\"presidio-analyzer\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load SpaCy model for NLP\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "nlp = spacy.load(spacy_model_path)\n",
        "\n",
        "# Configure NLP Engine with ignored labels\n",
        "nlp_engine = SpacyNlpEngine(\n",
        "    models=[{\n",
        "        \"lang_code\": \"en\",\n",
        "        \"model_name\": spacy_model_path,\n",
        "        \"labels_to_ignore\": [\"FAC\"]  # Ignore facility labels\n",
        "    }]\n",
        ")\n",
        "\n",
        "# Initialize Analyzer Engine with registry\n",
        "registry = RecognizerRegistry()\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
        "\n",
        "# Initialize Anonymizer Engine\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "def mask_pii(text, allowlist=None, denylist=None):\n",
        "    if allowlist is None:\n",
        "        allowlist = []\n",
        "    if denylist is None:\n",
        "        denylist = []\n",
        "\n",
        "    try:\n",
        "        # Analyze the text for PII entities\n",
        "        results = analyzer.analyze(\n",
        "            text=text,\n",
        "            language=\"en\",\n",
        "            score_threshold=0.5,\n",
        "            entities=None,  # Detect all supported entities\n",
        "            return_decision_process=True  # Include analysis explanation\n",
        "        )\n",
        "\n",
        "        # Filter out allowlisted entities\n",
        "        filtered_results = [\n",
        "            result for result in results\n",
        "            if result.entity_type not in allowlist and result.entity_type not in denylist\n",
        "        ]\n",
        "\n",
        "        # Anonymize the text\n",
        "        anonymized_text = anonymizer.anonymize(text=text, analyzer_results=filtered_results)\n",
        "\n",
        "        # Prepare findings for CSV\n",
        "        findings = []\n",
        "        for result in filtered_results:\n",
        "            entity_text = text[result.start:result.end]\n",
        "            finding = {\n",
        "                \"Entity_Type\": result.entity_type,\n",
        "                \"Text\": entity_text,\n",
        "                \"Start\": result.start,\n",
        "                \"End\": result.end,\n",
        "                \"Confidence\": result.score,\n",
        "                \"Source\": \"Presidio Analyzer\"  # Instead of undefined recognizer attribute\n",
        "            }\n",
        "            findings.append(finding)\n",
        "\n",
        "        # Create DataFrame and save to CSV\n",
        "        if findings:  # Only create CSV if there are findings\n",
        "            df = pd.DataFrame(findings)\n",
        "            df.to_csv('findings.csv', index=False)\n",
        "            print(f\"Found {len(findings)} PII entities\")\n",
        "        else:\n",
        "            print(\"No PII entities found\")\n",
        "\n",
        "        return anonymized_text.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during PII masking: {str(e)}\")\n",
        "        return text  # Return original text if error occurs\n",
        "\n",
        "# Test Input\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Specify entities to ignore (optional)\n",
        "ignore_entities = [\"FAC\"]  # Ignore facility entities\n",
        "\n",
        "# Run the function\n",
        "masked_text = mask_pii(input_text)\n",
        "print(\"\\nMasked Output:\\n\", masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EfHsA3sHmEJ",
        "outputId": "8840e6bf-e682-4bc7-d70d-34a77015f905"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during PII masking: Cannot save file into a non-existent directory: 'media'\n",
            "\n",
            "Masked Output:\n",
            " \n",
            "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig  # Correct operator configuration\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "import spacy\n",
        "from presidio_analyzer import RecognizerRegistry\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"PII_Masker\")\n",
        "\n",
        "# Load SpaCy model for NLP\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "nlp = spacy.load(spacy_model_path)\n",
        "\n",
        "# Configure NLP Engine with ignored labels\n",
        "nlp_engine = SpacyNlpEngine(\n",
        "    models=[{\n",
        "        \"lang_code\": \"en\",\n",
        "        \"model_name\": spacy_model_path,\n",
        "        \"labels_to_ignore\": [\"FAC\"]  # Ignore facility labels\n",
        "    }]\n",
        ")\n",
        "\n",
        "# Initialize Analyzer Engine with registry\n",
        "registry = RecognizerRegistry()\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
        "\n",
        "# Initialize Anonymizer Engine\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "def mask_pii(text, allowlist=None, denylist=None, custom_replacement=None):\n",
        "    \"\"\"\n",
        "    Detects and anonymizes PII entities in the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text containing potential PII.\n",
        "        allowlist (list): Entity types to skip anonymization.\n",
        "        denylist (list): Entity types to ignore completely.\n",
        "        custom_replacement (dict): Custom replacements for specific entity types.\n",
        "\n",
        "    Returns:\n",
        "        str: Anonymized text.\n",
        "    \"\"\"\n",
        "    if allowlist is None:\n",
        "        allowlist = []\n",
        "    if denylist is None:\n",
        "        denylist = []\n",
        "    if custom_replacement is None:\n",
        "        custom_replacement = {}\n",
        "\n",
        "    try:\n",
        "        # Analyze the text for PII entities\n",
        "        logger.info(\"Analyzing text for PII...\")\n",
        "        results = analyzer.analyze(\n",
        "            text=text,\n",
        "            language=\"en\",\n",
        "            score_threshold=0.5,\n",
        "            entities=None,  # Detect all supported entities\n",
        "            return_decision_process=False\n",
        "        )\n",
        "\n",
        "        # Filter out allowlisted and denylisted entities\n",
        "        filtered_results = [\n",
        "            result for result in results\n",
        "            if result.entity_type not in allowlist and result.entity_type not in denylist\n",
        "        ]\n",
        "\n",
        "        # Prepare operators for anonymization\n",
        "        operators = {}\n",
        "        for result in filtered_results:\n",
        "            entity_type = result.entity_type\n",
        "            replacement = custom_replacement.get(entity_type, f\"<{entity_type}>\")\n",
        "            operators[entity_type] = OperatorConfig(\"replace\", {\"new_value\": replacement})\n",
        "\n",
        "        # Anonymize the text\n",
        "        anonymized_text = anonymizer.anonymize(\n",
        "            text=text,\n",
        "            analyzer_results=filtered_results,\n",
        "            operators=operators\n",
        "        )\n",
        "\n",
        "        # Prepare findings for CSV\n",
        "        findings = []\n",
        "        for result in filtered_results:\n",
        "            entity_text = text[result.start:result.end]\n",
        "            finding = {\n",
        "                \"Entity_Type\": result.entity_type,\n",
        "                \"Text\": entity_text,\n",
        "                \"Start\": result.start,\n",
        "                \"End\": result.end,\n",
        "                \"Confidence\": result.score,\n",
        "                \"Source\": \"Presidio Analyzer\"\n",
        "            }\n",
        "            findings.append(finding)\n",
        "\n",
        "        # Create DataFrame and save to CSV\n",
        "        if findings:  # Only create CSV if there are findings\n",
        "            df = pd.DataFrame(findings)\n",
        "            csv_file = 'findings.csv'\n",
        "            df.to_csv(csv_file, index=False)\n",
        "            logger.info(f\"PII findings saved to {csv_file}\")\n",
        "            print(\"\\nCSV Content:\\n\", df.to_string(index=False))\n",
        "        else:\n",
        "            logger.info(\"No PII entities found.\")\n",
        "\n",
        "        return anonymized_text.text\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during PII masking: {str(e)}\", exc_info=True)\n",
        "        return text  # Return original text if error occurs\n",
        "\n",
        "\n",
        "# Test Input\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Specify entities to ignore or customize replacements\n",
        "ignore_entities = [\"FAC\"]  # Ignore facility entities\n",
        "custom_replacements = {\"PERSON\": \"XXXX\", \"LOCATION\": \"Hidden\"}\n",
        "\n",
        "# Run the function\n",
        "masked_text = mask_pii(input_text, custom_replacement=custom_replacements)\n",
        "print(\"\\nMasked Output:\\n\", masked_text)\n"
      ],
      "metadata": {
        "id": "eIIbGcT4NV8R",
        "outputId": "4982bc77-c5ff-4c87-c305-df4a89567b75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CSV Content:\n",
            " Entity_Type           Text  Start  End  Confidence            Source\n",
            "     PERSON       John Doe      1    9        0.85 Presidio Analyzer\n",
            "   LOCATION    Springfield     35   46        0.85 Presidio Analyzer\n",
            "  DATE_TIME 12th July 1990    116  130        0.85 Presidio Analyzer\n",
            "  DATE_TIME   9 AM to 5 PM    146  158        0.85 Presidio Analyzer\n",
            "\n",
            "Masked Output:\n",
            " \n",
            "XXXX lives at 123 Elm Street, Hidden. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on <DATE_TIME> and works from <DATE_TIME>.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine, Pattern, RecognizerResult\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "from presidio_analyzer import RecognizerRegistry, PatternRecognizer\n",
        "import spacy\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"PII_Masker\")\n",
        "\n",
        "# Load SpaCy model for NLP\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "nlp = spacy.load(spacy_model_path)\n",
        "\n",
        "# Configure NLP Engine\n",
        "nlp_engine = SpacyNlpEngine(models=[{\"lang_code\": \"en\", \"model_name\": spacy_model_path}])\n",
        "\n",
        "# Initialize Analyzer Engine\n",
        "registry = RecognizerRegistry()\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine, registry=registry)\n",
        "\n",
        "# Initialize Anonymizer Engine\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# Define custom recognizer for detecting addresses\n",
        "class AddressRecognizer(PatternRecognizer):\n",
        "    def __init__(self):\n",
        "        patterns = [\n",
        "            Pattern(\n",
        "                name=\"address_pattern\",\n",
        "                regex=r\"\\d{1,5}\\s[A-Za-z0-9\\s]+(?:Street|St|Avenue|Ave|Boulevard|Blvd|Road|Rd)\\b\",\n",
        "                score=0.85,\n",
        "            )\n",
        "        ]\n",
        "        super().__init__(supported_entity=\"ADDRESS\", patterns=patterns)\n",
        "\n",
        "# Add custom recognizer to registry\n",
        "address_recognizer = AddressRecognizer()\n",
        "registry.add_recognizer(address_recognizer)\n",
        "\n",
        "def mask_pii(text, allowlist=None, denylist=None, custom_replacement=None, chunk_size=1000):\n",
        "    if allowlist is None:\n",
        "        allowlist = []\n",
        "    if denylist is None:\n",
        "        denylist = []\n",
        "    if custom_replacement is None:\n",
        "        custom_replacement = {}\n",
        "\n",
        "    try:\n",
        "        findings = []\n",
        "        anonymized_texts = []\n",
        "        start_index = 0\n",
        "\n",
        "        # Process text in chunks\n",
        "        while start_index < len(text):\n",
        "            chunk = text[start_index:start_index + chunk_size]\n",
        "            logger.info(f\"Processing chunk: {start_index}-{start_index + chunk_size}\")\n",
        "\n",
        "            # Analyze text for PII entities\n",
        "            results = analyzer.analyze(\n",
        "                text=chunk,\n",
        "                language=\"en\",\n",
        "                score_threshold=0.5,\n",
        "                entities=None,  # Detect all supported entities\n",
        "                return_decision_process=False,\n",
        "            )\n",
        "\n",
        "            # Filter results based on allowlist/denylist\n",
        "            filtered_results = [\n",
        "                result for result in results\n",
        "                if result.entity_type not in allowlist and result.entity_type not in denylist\n",
        "            ]\n",
        "\n",
        "            # Configure anonymization operators\n",
        "            operators = {}\n",
        "            for result in filtered_results:\n",
        "                entity_type = result.entity_type\n",
        "                replacement = custom_replacement.get(entity_type, f\"<{entity_type}>\")\n",
        "                operators[entity_type] = OperatorConfig(\"replace\", {\"new_value\": replacement})\n",
        "\n",
        "            # Anonymize text\n",
        "            anonymized_chunk = anonymizer.anonymize(\n",
        "                text=chunk,\n",
        "                analyzer_results=filtered_results,\n",
        "                operators=operators,\n",
        "            )\n",
        "            anonymized_texts.append(anonymized_chunk.text)\n",
        "\n",
        "            # Prepare findings for CSV\n",
        "            for result in filtered_results:\n",
        "                entity_text = chunk[result.start:result.end]\n",
        "                finding = {\n",
        "                    \"Entity_Type\": result.entity_type,\n",
        "                    \"Text\": entity_text,\n",
        "                    \"Start\": result.start + start_index,\n",
        "                    \"End\": result.end + start_index,\n",
        "                    \"Confidence\": result.score,\n",
        "                    \"Source\": \"Presidio Analyzer\",\n",
        "                }\n",
        "                findings.append(finding)\n",
        "\n",
        "            # Update start index for next chunk\n",
        "            start_index += chunk_size\n",
        "\n",
        "        # Save findings to CSV\n",
        "        if findings:\n",
        "            df = pd.DataFrame(findings)\n",
        "            csv_file = \"findings.csv\"\n",
        "            df.to_csv(csv_file, index=False)\n",
        "            logger.info(f\"PII findings saved to {csv_file}\")\n",
        "            print(\"\\nCSV Content:\\n\", df.to_string(index=False))\n",
        "        else:\n",
        "            logger.info(\"No PII entities found.\")\n",
        "\n",
        "        # Return concatenated anonymized text\n",
        "        return \" \".join(anonymized_texts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during PII masking: {str(e)}\", exc_info=True)\n",
        "        return text  # Return original text if error occurs\n",
        "\n",
        "# Test Input\n",
        "input_text = \"\"\"\n",
        "John Doe lives at 123 Elm Street, Springfield. His company, TechCorp, pays him a salary of $50,000.\n",
        "He was born on 12th July 1990 and works from 9 AM to 5 PM.\n",
        "\"\"\" *2  # Simulate a large file by repeating the text\n",
        "\n",
        "# Custom replacements\n",
        "custom_replacements = {\"PERSON\": \"XXXX\", \"LOCATION\": \"Hidden\", \"ADDRESS\": \"<ADDRESS>\"}\n",
        "\n",
        "# Run the function\n",
        "masked_text = mask_pii(input_text, custom_replacement=custom_replacements, chunk_size=500)\n",
        "print(\"\\nMasked Output:\\n\", masked_text)\n"
      ],
      "metadata": {
        "id": "cub-e9g2ocPG",
        "outputId": "f8a72742-45b6-4c02-d761-33e3c9e53917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Entity FAC is not mapped to a Presidio entity, but keeping anyway. Add to `NerModelConfiguration.labels_to_ignore` to remove.\n",
            "WARNING:presidio-analyzer:Entity FAC is not mapped to a Presidio entity, but keeping anyway. Add to `NerModelConfiguration.labels_to_ignore` to remove.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CSV Content:\n",
            " Entity_Type           Text  Start  End  Confidence            Source\n",
            "     PERSON       John Doe      1    9        0.85 Presidio Analyzer\n",
            "    ADDRESS 123 Elm Street     19   33        0.85 Presidio Analyzer\n",
            "   LOCATION    Springfield     35   46        0.85 Presidio Analyzer\n",
            "  DATE_TIME 12th July 1990    116  130        0.85 Presidio Analyzer\n",
            "  DATE_TIME   9 AM to 5 PM    146  158        0.85 Presidio Analyzer\n",
            "     PERSON           John    161  165        0.85 Presidio Analyzer\n",
            "    ADDRESS 123 Elm Street    179  193        0.85 Presidio Analyzer\n",
            "   LOCATION    Springfield    195  206        0.85 Presidio Analyzer\n",
            "  DATE_TIME 12th July 1990    276  290        0.85 Presidio Analyzer\n",
            "  DATE_TIME   9 AM to 5 PM    306  318        0.85 Presidio Analyzer\n",
            "\n",
            "Masked Output:\n",
            " \n",
            "XXXX lives at <ADDRESS>, Hidden. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on <DATE_TIME> and works from <DATE_TIME>.\n",
            "\n",
            "XXXX Doe lives at <ADDRESS>, Hidden. His company, TechCorp, pays him a salary of $50,000.\n",
            "He was born on <DATE_TIME> and works from <DATE_TIME>.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig\n",
        "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
        "import spacy\n",
        "from docx import Document  # For handling Word documents\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"PII_Masker\")\n",
        "\n",
        "# Load SpaCy model\n",
        "spacy_model_path = \"en_core_web_sm\"\n",
        "nlp = spacy.load(spacy_model_path)\n",
        "\n",
        "nlp_engine = SpacyNlpEngine(\n",
        "    models=[{\n",
        "        \"lang_code\": \"en\",  # Language code\n",
        "        \"model_name\": \"en_core_web_sm\",  # Name of the SpaCy model\n",
        "        \"labels_to_ignore\": [\"FAC\"]  # Optional: Ignore specific labels\n",
        "    }]\n",
        ")\n",
        "\n",
        "# Initialize Presidio engines\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine)  # Attach NLP engine\n",
        "anonymizer = AnonymizerEngine()\n",
        "# Initialize Presidio engines\n",
        "analyzer = AnalyzerEngine(nlp_engine=nlp_engine)\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "def mask_pii(text, custom_replacement=None, chunk_size=1000):\n",
        "    if custom_replacement is None:\n",
        "        custom_replacement = {}\n",
        "\n",
        "    try:\n",
        "        findings = []\n",
        "        anonymized_texts = []\n",
        "        start_index = 0\n",
        "\n",
        "        while start_index < len(text):\n",
        "            chunk = text[start_index:start_index + chunk_size]\n",
        "            logger.info(f\"Processing chunk: {start_index}-{start_index + chunk_size}\")\n",
        "\n",
        "            # Analyze text for PII entities\n",
        "            results = analyzer.analyze(\n",
        "                text=chunk,\n",
        "                language=\"en\",\n",
        "                score_threshold=0.5\n",
        "            )\n",
        "\n",
        "            # Configure anonymization operators\n",
        "            operators = {}\n",
        "            for result in results:\n",
        "                entity_type = result.entity_type\n",
        "                replacement = custom_replacement.get(entity_type, f\"<{entity_type}>\")\n",
        "                operators[entity_type] = OperatorConfig(\"replace\", {\"new_value\": replacement})\n",
        "\n",
        "            # Anonymize text\n",
        "            anonymized_chunk = anonymizer.anonymize(\n",
        "                text=chunk,\n",
        "                analyzer_results=results,\n",
        "                operators=operators\n",
        "            )\n",
        "            anonymized_texts.append(anonymized_chunk.text)\n",
        "\n",
        "            # Prepare findings for CSV\n",
        "            for result in results:\n",
        "                entity_text = chunk[result.start:result.end]\n",
        "                finding = {\n",
        "                    \"Entity_Type\": result.entity_type,\n",
        "                    \"Text\": entity_text,\n",
        "                    \"Start\": result.start + start_index,\n",
        "                    \"End\": result.end + start_index,\n",
        "                    \"Confidence\": result.score,\n",
        "                    \"Source\": \"Presidio Analyzer\",\n",
        "                }\n",
        "                findings.append(finding)\n",
        "\n",
        "            start_index += chunk_size\n",
        "\n",
        "        # Save findings to CSV\n",
        "        if findings:\n",
        "            df = pd.DataFrame(findings)\n",
        "            csv_file = \"findings.csv\"\n",
        "            df.to_csv(csv_file, index=False)\n",
        "            logger.info(f\"PII findings saved to {csv_file}\")\n",
        "\n",
        "        return \" \".join(anonymized_texts)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during PII masking: {str(e)}\", exc_info=True)\n",
        "        return text\n",
        "\n",
        "def process_file(input_file, custom_replacement=None):\n",
        "    try:\n",
        "        file_name, file_ext = os.path.splitext(input_file)\n",
        "        if file_ext.lower() == \".txt\":\n",
        "            # Process text file\n",
        "            with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            masked_text = mask_pii(text, custom_replacement=custom_replacement)\n",
        "\n",
        "            # Save masked text\n",
        "            output_file = f\"{file_name}_masked.txt\"\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(masked_text)\n",
        "            logger.info(f\"Masked text saved to {output_file}\")\n",
        "\n",
        "        elif file_ext.lower() == \".docx\":\n",
        "            # Process Word document\n",
        "            doc = Document(input_file)\n",
        "            text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "            masked_text = mask_pii(text, custom_replacement=custom_replacement)\n",
        "\n",
        "            # Save masked document\n",
        "            output_doc = Document()\n",
        "            for line in masked_text.split(\"\\n\"):\n",
        "                output_doc.add_paragraph(line)\n",
        "            output_file = f\"{file_name}_masked.docx\"\n",
        "            output_doc.save(output_file)\n",
        "            logger.info(f\"Masked document saved to {output_file}\")\n",
        "\n",
        "        else:\n",
        "            logger.error(\"Unsupported file type. Only .txt and .docx are supported.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing file {input_file}: {str(e)}\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage: replace this with your file name\n",
        "    input_file = \"input.txt\"  # Change this to your file name\n",
        "    custom_replacement = {\"PERSON\": \"XXXX\", \"LOCATION\": \"Hidden\"}\n",
        "    process_file(input_file, custom_replacement=custom_replacement)\n"
      ],
      "metadata": {
        "id": "6HVlWqSFpzAu",
        "outputId": "0a7bd4de-221c-43c2-a59e-cff0e03b8d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:model_to_presidio_entity_mapping is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:low_score_entity_names is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:labels_to_ignore is missing from configuration, using default\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "NkINet1uOyc1",
        "outputId": "820073a4-f59a-4a64-f593-5ebb9746d149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/244.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IT-hGv-8O2Rw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}